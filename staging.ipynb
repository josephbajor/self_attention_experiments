{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchexplorer as te\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from src.model.LM import SimpleBigramModel, AttentionLM\n",
    "from src.dataloaders import build_loaders\n",
    "from hparams import Hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Hparams()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, train_loader, _, _ = build_loaders(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionLM(hparams, vocab_size=tokenizer.get_vocab_size()).to(\"cuda\")\n",
    "model.compile()\n",
    "\n",
    "# te.watch(model, log_freq=1, log=['io',  'io_grad', 'params', 'params_grad'], backend='standalone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionLM(\n",
      "  (embed): Embedding(1965, 512)\n",
      "  (embed_pos): Embedding(128, 512)\n",
      "  (attention): ModuleList(\n",
      "    (0-3): 4 x AttentionBlock(\n",
      "      (attention_heads): ModuleList(\n",
      "        (0-2): 3 x FullAttention(\n",
      "          (q): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (k): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (v): Linear(in_features=512, out_features=256, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (3): Dropout(p=0.08, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=1965, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8112045"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(\n",
    "    model.parameters(), lr=hparams.lr, weight_decay=hparams.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.67247:   1%|          | 6537/1228004 [35:18<105:20:27,  3.22it/s]     "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m windowed_loss[idx\u001b[38;5;241m%\u001b[39mloss_buffer] \u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m---> 29\u001b[0m bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mwindowed_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m bar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# save training checkpoint\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/numpy/core/_methods.py:162\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _clip_dep_invoke_with_casting(\n\u001b[1;32m    160\u001b[0m             um\u001b[38;5;241m.\u001b[39mclip, a, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m, out\u001b[38;5;241m=\u001b[39mout, casting\u001b[38;5;241m=\u001b[39mcasting, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mean\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    163\u001b[0m     arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[1;32m    165\u001b[0m     is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_buffer = 100\n",
    "save_pth = f\"checkpoints/{str(uuid.uuid4())[:8]}\"\n",
    "Path(f\"{save_pth}\").mkdir(exist_ok=True, parents=True)\n",
    "hparams.save_to_file(f\"{save_pth}/hparams.json\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(hparams.epochs):\n",
    "    windowed_loss = np.zeros(loss_buffer, dtype=np.float32)\n",
    "    bar = tqdm(total=len(train_loader))\n",
    "\n",
    "    for idx, (x, y) in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        x = x.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        \n",
    "        logits = model(x)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        y = y.view(B*T)\n",
    "\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        windowed_loss[idx%loss_buffer] = loss\n",
    "\n",
    "        bar.set_description(f\"Loss: {windowed_loss.mean():.5f}\")\n",
    "        bar.update()\n",
    "\n",
    "        if idx % 10000 == 0:\n",
    "            # save training checkpoint\n",
    "            torch.save(model.state_dict(), f\"{save_pth}/model.pth\")\n",
    "        \n",
    "    bar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input:str, model, tokenizer, out_len:int, determenistic=False):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        input = torch.tensor(tokenizer.encode(input).ids, dtype=torch.long).to('cuda')\n",
    "        out = model.generate_batch(input, out_len, deterministic=determenistic)\n",
    "        out = [tokenizer.decode(list(t)) for t in out]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inference(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mmodel\u001b[49m, tokenizer, out_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "inference('Hello', model, tokenizer, out_len=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- Query comes from other sequence (or the sequence itself in self attention) and is a value that converges to represent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
